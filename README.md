# Group 3: The evaluation of Java-Python code translation using scraped and generated datasets

This repository provides a way to fine-tune 2 models ([CodeBERT](https://github.com/microsoft/CodeBERT) and [PLBART](https://github.com/wasiahmad/PLBART)) on a scraped and generated dataset and evaluate these fine-tuned models on two metrics, namely Bleu-4 and xMatch. 
The evaluation and fine-tuning of the models has been n/adapted from [CodeXGLUE](https://github.com/microsoft/CodeXGLUE) and the preprocessing of the data is done by [AVATAR](https://github.com/wasiahmad/AVATAR).


# Dataset

The dataset consists of data scraped from [Rosetta Code](https://rosettacode.org/wiki/Rosetta_Code), [Sample Programs](https://sampleprograms.io/languages/), [Coding Bat](https://codingbat.com/), [A problem solved in 22 programming languages](https://andrewshitov.com/2020/12/07/a-problem-solved-in-22-programming-languages/) and [LeetCode](https://walkccc.me/LeetCode/).
Furthermore, it has been extended with data generated by the [OpenAI Codex](https://openai.com/blog/openai-codex/).


# Installation

After cloning a `pip install -r requirements.txt` is required to install the appropriate packages for running the fine-tuning and evaluation.


### Code Scraping

From the `data_gathering` directory, any of the scrapers can be run through the command `scrapy crawl sitename`, where `sitename` is one of `rosettacode`, `sampleprograms`, or `walkccc`.

All scrapers can be configured with the following options:
- `langs` specifies which languages the scraper should look for
  - only problems implemented in all specified languages will be retrieved
  - currently supported languages: `Java`, `Python`, and `C++`
  - default value: `Java,Python` for all except `walkccc` where it is `C++, Java, Python`
  - example usage: `scrapy crawl sitename -a langs=Java,Python`
- `res_dir` specifies the output directory
  - by default, the `data` directory within the repository root is used
  - example usage: `scrapy crawl sitename -a res_dir="../data"`

Additionally, the `rosettacode` parser has the `rm_drafts` option, which, if set to value `y` (as by default), will not save [draft tasks](https://rosettacode.org/wiki/Category:Draft_Programming_Tasks). Any other value will retain drafts.

Under the aforementioned `data` directory, is a corresponding output for each of the scrapers (`rosettacode` was run with drafts retained, default values were used for all other parameters). Data under `codingbat` and `andrewshitov` was collected manually.


### Data Generation

Synthetic data can be generated with the help of the `code_generator.py` script in the `data_gathering` directory. As it uses the [OpenAI API](https://openai.com/api/), the `OPENAI_API_KEY` environment variable must exist and be valid for successful execution.

**Please keep in mind that usage of the OpenAI API generally incurs a cost that will be billed to the account/organization tied to the supplied key**.

Running `python code_generator.py -h` gives an overview of all available configuration options alongside their default values.

As before, the `data` directory holds an output instance of running the generator on the manually curated data from [CoNaLa](https://conala-corpus.github.io/) (both train and test). Prompts were taken from the `intent_field` (another option would be `rewritten_intent`). The input data is also present under `resources/conala-corpus-v1.1`.


### Misc

In the case of both the scrapers and the generator, the currently supported languages are: `C++`, `Java`, `Python`. Adding support for a new language requires updating the `LANG_EXTS` dictionary from `data_gathering/langs_util.py`, which specifies the file extension for each recognized language.

The class `CodeChecker` under `data_gathering/langs_util.py` is currently unused within the scraper or generator, but it can be used to checked whether the program stored in a string is syntactically valid through the `is_invalid` instance method. It supports the same languages as the other two tools. In this instance, adding support for more languages would also require adding an appropriate [Tree-sitter](https://tree-sitter.github.io/tree-sitter/) language [parsers](https://tree-sitter.github.io/tree-sitter/#available-parsers).


### Pre-processing

The pre-processing is done by first setting `output_file` (indicating the location and name of the output file) and setting `from_directory` (indicating the directory containing the java-python files).
**Note that the python and java files should be in the same directory and have the same name (with the `.java` and `.py` extension respectively)**

The pre-processing can then be done by running the following commands (requires conda):
```shell
cd ./fine_tuning
git clone https://github.com/wasiahmad/AVATAR.git
cd ./AVATAR
install_env.sh
pip install -r requirements.txt
cd ../../
python3 ./fine_tuning/process.py
```

### Fine-tuning

A script for running the evaluation is provided which can be run using:
```bash
./fine_tuning/train_and_test.java roberta # or plbart
```
This will run the training with the settings provided in the script file (default of 50000 training steps and a batch size of 4). To change from Java-Python translation to Python-Java translation, switch the order of the files given to `--train_filename` and `dev_filename`.


# Results

Java to Python Translation:
| Name                 | Bleu-4 | xMatch |
|----------------------|--------|--------|
| CodeBERT             | 51.33  | 0.0    |
| PLBART               | 62.8   | **2.5126** |
| CodeBERT (Augmented) | 49.57  | 0.5025 |
| PLBART (Augmented)   | **63.44**  | **2.5126** |

Python to Java Translation:
| Name                 | Bleu-4 | xMatch |
|----------------------|--------|--------|
| CodeBERT             | 46.78  | 0.0    |
| PLBART               | **58.43**  | **3.0151** |
| CodeBERT (Augmented) | 45.42  | 0.0    |
| PLBART (Augmented)   | 49.86  | 1.5075 |
