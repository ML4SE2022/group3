# Group 3: The evaluation of Java-Python code translation using scraped and generated datasets

This repository provides a way to fine-tune 2 models ([CodeBERT](https://github.com/microsoft/CodeBERT) and [PLBART](https://github.com/wasiahmad/PLBART)) on a scraped and generated dataset and evaluate these fine-tuned models on two metrics, namely Bleu-4 and xMatch.
The evaluation and fine-tuning of the models has been n/adapted from [CodeXGLUE](https://github.com/microsoft/CodeXGLUE) and the preprocessing of the data is done by [AVATAR](https://github.com/wasiahmad/AVATAR).


# Dataset

The dataset consists of data scraped from [Rosetta Code](https://rosettacode.org/wiki/Rosetta_Code), [Sample Programs](https://sampleprograms.io/languages/), [Coding Bat](https://codingbat.com/), [A problem solved in 22 programming languages](https://andrewshitov.com/2020/12/07/a-problem-solved-in-22-programming-languages/) and [LeetCode](https://walkccc.me/LeetCode/).
Furthermore, it has been extended with data generated by the [OpenAI Codex](https://openai.com/blog/openai-codex/).

The datasets used for experimentation can be found in the folder `train_test_data`, the training set including generated data is found in the files `augmented_train.[py|java]`. Running the pre-processing might result in a slightly different resulting dataset as the pre-processing has been slightly adjusted.


# Installation

After cloning a `pip install -r requirements.txt` is required to install the appropriate packages for running the fine-tuning and evaluation (preferably within a [venv](https://docs.python.org/3/library/venv.html)).


### Code Scraping

From the `data_gathering` directory, any of the scrapers can be run through the command `scrapy crawl sitename`, where `sitename` is one of `rosettacode`, `sampleprograms`, or `walkccc`.

All scrapers can be configured with the following options:
- `langs` specifies which languages the scraper should look for
  - only problems implemented in all specified languages will be retrieved
  - currently supported languages: `Java`, `Python`, and `C++`
  - default value: `Java,Python` for all except `walkccc` where it is `C++, Java, Python`
  - example usage: `scrapy crawl sitename -a langs=Java,Python`
- `res_dir` specifies the output directory
  - by default, the `data` directory within the repository root is used
  - example usage: `scrapy crawl sitename -a res_dir="../data"`

Additionally, the `rosettacode` parser has the `rm_drafts` option, which, if set to value `y` (as by default), will not save [draft tasks](https://rosettacode.org/wiki/Category:Draft_Programming_Tasks). Any other value will retain drafts.

Under the aforementioned `data` directory, is a corresponding output for each of the scrapers (`rosettacode` was run with drafts retained, default values were used for all other parameters). Data under `codingbat` and `andrewshitov` was collected manually.


### Data Generation

Synthetic data can be generated with the help of the `code_generator.py` script in the `data_gathering` directory. As it uses the [OpenAI API](https://openai.com/api/), the `OPENAI_API_KEY` environment variable must exist and be valid for successful execution.

**Please keep in mind that usage of the OpenAI API generally incurs a cost that will be billed to the account/organization tied to the supplied key**.

Running `python code_generator.py -h` gives an overview of all available configuration options alongside their default values.

As before, the `data` directory holds an output instance of running the generator on the manually curated data from [CoNaLa](https://conala-corpus.github.io/) (both train and test). Prompts were taken from the `intent_field` (another option would be `rewritten_intent`). The input data is also present under `resources/conala-corpus-v1.1`.


### Misc

In the case of both the scrapers and the generator, the currently supported languages are: `C++`, `Java`, `Python`. Adding support for a new language requires updating the `LANG_EXTS` dictionary from `data_gathering/langs_util.py`, which specifies the file extension for each recognized language.

The class `CodeChecker` under `data_gathering/langs_util.py` is currently unused within the scraper or generator, but it can be used to checked whether the program stored in a string is syntactically valid through the `is_invalid` instance method. It supports the same languages as the other two tools. In this instance, adding support for more languages would also require adding an appropriate [Tree-sitter](https://tree-sitter.github.io/tree-sitter/) language [parsers](https://tree-sitter.github.io/tree-sitter/#available-parsers).


### Pre-processing

The pre-processing is done by first setting `output_file` (indicating the location and name of the output file) and setting `from_directory` (indicating the directory containing the java-python files).
**Note that the python and java files should be in the same directory and have the same name (with the `.java` and `.py` extension respectively).**

The pre-processing can then be done by running the following commands:
```shell
cd ./fine_tuning
python3 process.py
```

### Fine-tuning

A script for running the evaluation is provided which can be run using:
```bash
./fine_tuning/train_and_test.sh roberta # or plbart
```
This will run the training with the settings provided in the script file (default of 50000 training steps and a batch size of 4). To change from Java-Python translation to Python-Java translation, switch the order of the files given to `--train_filename` and `dev_filename`.

# Model Artefacts

All models and outputs of evaluation can be found [here](https://drive.google.com/file/d/1hMEBrahXkBQ6mhLK4YXjwM0MIvajrXVb/view?usp=sharing). There are two folder corresponding to Java to Python Translation (JavaPython) and Python to Java Translation (PythonJava).
Each of the zips within this folder contains the output folder of the training and testing run by `./fine_tuning/train_and_test.sh`.
The `test_0.output` file contains the translations of the provided test set. In the `checkpoint-best-ppl` folder within each zip is contained the final fine-tuned model. To evaluate these models, use the following command (from `./fine_tuning/train_and_test.sh`):
```bash
python3 run.py \
  --do_test \
	--model_type $model_type \
	--model_name_or_path $pretrained_model \
	--config_name $config_and_token_name \
	--tokenizer_name $config_and_token_name  \
	--load_model_path $output_dir/checkpoint-best-ppl/pytorch_model.bin \
	--dev_filename ../train_test_data/test.java,../train_test_data/test.py \
	--output_dir $output_dir \
	--max_source_length 512 \
	--max_target_length 512 \
	--beam_size 5 \
	--eval_batch_size 16
```
Where `--load_model_path` should correspond to the bin file of the model that you would like to run.


# Results

Java to Python Translation:
| Name                 | Bleu-4 | xMatch |
|----------------------|--------|--------|
| CodeBERT             | 51.33  | 0.0    |
| PLBART               | 62.8   | **2.5126** |
| CodeBERT (Augmented) | 49.57  | 0.5025 |
| PLBART (Augmented)   | **63.44**  | **2.5126** |

Python to Java Translation:
| Name                 | Bleu-4 | xMatch |
|----------------------|--------|--------|
| CodeBERT             | 46.78  | 0.0    |
| PLBART               | **58.43**  | **3.0151** |
| CodeBERT (Augmented) | 45.42  | 0.0    |
| PLBART (Augmented)   | 49.86  | 1.5075 |
